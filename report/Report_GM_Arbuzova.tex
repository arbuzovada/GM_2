\documentclass[12pt,a4paper,oneside,fleqn,leqno]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{epstopdf}
%\usepackage[center]{caption}
\geometry{a4paper}
\usepackage[russian]{babel}

\setlength{\topmargin}{-1.0in}
\setlength{\textheight}{10.5in}
\setlength{\oddsidemargin}{-.125in}
\setlength{\textwidth}{6.5in}

\begin{document}
	\begin{titlepage}
		\begin{center}
			\large Московский государственный университет им.\,М.\,В.\,Ломоносова\\
			Факультет вычислительной математики и кибернетики \\
			Кафедра математических методов прогнозирования \\[4.5cm] 
			\Huge Задание \No\,2.\\Низкоплотностные коды\\[5.5cm]
		\end{center}
		\normalsize
		\begin{flushright}
			\emph{Автор:} Арбузова Дарья\\
			\emph{Группа:} 417\\
		\end{flushright}
		\vfill
		\begin{center}
			Москва, 2014
		\end{center}
	\end{titlepage}
	\tableofcontents
	\newpage
	\section{Цели задания}
		\begin{enumerate}
			\item
			Реализовать алгоритм декодирования низкоплотностного кода на основе loopy BP;
			\item
			Провести эксперименты с различными расписаниями пересчёта сообщений и коэффициентами дэмпфирования;
			\item
			Реализовать алгоритм оценки вероятности битовой и блоковой ошибки кода с помощью метода статистических испытаний;
			\item
			Провести эксперименты по оцениванию битовой и блоковой ошибки низкоплотностного кода для различных значений параметров;
			\item
			Провести эксперименты по сравнению низкоплотностного кода с кодами БЧХ.
		\end{enumerate}
	\section{Теория помехоустойчивого кодирования}
		Рассмотрим решение задачи безошибочной передачи потока битовой информации по каналу с шумом с помощью кодов, исправляющих ошибки.\par
		Пусть передаваемое сообщение разделяется на блоки длины $k,$ и каждый из них кодируется независимо. Для исправления возможных помех при передаче информации необходимо добавить избыточность: пусть каждый блок кодируется словом длины $n > k.$ Таким образом, каждому возможному блоку длины $k$ сопоставляется одно из $2 ^ k$ кодовых слов длины $n$. Множество этих слов называется $(n, k)-$блоковым кодом, а $r = \frac{k}{n}$ --- скоростью кода.\par
		Стадии жизни сообщения:
		$$
			u \in \{0, 1\}^k \xrightarrow[\text{вание}]{\text{кодиро-}} v \in \{0, 1\}^n \xrightarrow[\text{помехами}]{\text{канал с}} w \in \{0, 1\}^n \xrightarrow[\text{ошибок}]{\text{восстановление}} \hat{v} \in \{0, 1\}^n \xrightarrow[\text{рование}]{\text{декоди-}} \hat{u} \in \{0, 1\}^k
		$$\par
		Рассмотрим следующую модель канала: пусть ошибка в каждом бите совершается с вероятностью $q \in \left (0; \frac{1}{2}\right).$ Пропускной способностью канала называется максимальная скорость, с которой может быть осуществлена надёжная передачи информации. В рассматриваемом случае определяется величиной $c = 1 + q\log_2q + (1 - q)\log_2(1 - q).$\par
		Зададим линейный $(n, k)-$блоковый код его проверочной матрицей $H \in \{0, 1\}^{(n - k)\times n}: Hv = 0\,(\text{mod}\,\,2) \iff v$ --- кодовое слово. По ней можно найти порождающую матрицу кода для кодирования блоков $G \in \{0, 1\}^{n\times k}: Gu=v.$ Подробное описание алгоритма можно найти в [1].\par
		Особенностью низкоплотностных кодов является сильная разреженность матрицы $H.$\par
		Пусть получено сообщение $w \in \{0, 1\}^n$, и требуется восстановить вектор ошибок $e\in \{0, 1\}^n: w = v + e.$ Назовём синдромом $w$ вектор $s \in \{0, 1\}^{n - k}: s = Hw = H(v + e) =\,= Hv + He = He.$ \par
		Основная задача декодирования состоит в решении уравнения $s = He,$ и делать это можно многими разными способами. В данном задании был исследован метод, использующий аппарат графических моделей.\par
		При использовании побитовой функции потерь $\lambda(e, \tilde e) = \sum\limits_{i = 1}^n[e_i \neq \tilde e_i]$ оптимальная процедура декодирования связана с максимизацией маргиналов отдельных переменных: $\hat{e}_i = \text{arg} \max\limits_{e_i}p(e_i|s).$
Для поиска маргинальных распределений $p(e_i|s)$ применяется алгоритм sum-product loopy belief propagation на фактор-графе.\par
		В результате работы алгоритма возможны 3 ситуации:
		\begin{enumerate}
			\itemsep0em
			\item
			Найден вектор ошибок $e,$ удовлетворяющий решаемому уравнению;
			\item
			Произошла стабилизация оценок на маргинальные распределения;
			\item
			Достигнуто максимальное число итераций.
		\end{enumerate}\par
		Отметим, что ни один из этих вариантов не гарантирует правильного (или неправильного) результата декодирования сообщения.\par
		При передаче сообщений в алгоритме возможны две схемы: параллельное расписание (сначала все вершины посылают сообщения во все факторы, а затем все факторы --- во все вершины) и последовательное (на каждой итерации алгоритма сообщения обновляются в случайном порядке).\par
		Ещё одной модификацией алгоритма является демпфирование, когда сообщение обновляется выпуклой комбинацией старого сообщения и пересчитанного нового: $\mu^{t + 1} =\,\,\,= \lambda \mu_{new} + (1 - \lambda)\mu^{t}, \lambda \in (0;1],$ иными словами, происходит экспоненциальное сглаживание значений.
		\par
		В экспериментах ниже будет рассмотрено общее поведение представленного алгоритма декодирования, а также влияние различных параметров на качество его работы.
	\section{Эксперименты}
		\subsection{Различные расписания и коэффициенты демпфирования}
			Исследуем поведение алгоритма в зависимости от расписания и коэффициента демпфирования.\par
			Зафиксируем параметры $n = 50, k = 10, q = 0.1$ и оценим время работы алгоритма декодирования сообщений. Результаты представлены в таблице \ref{tab:lambda}:
			\begin{table}[H]
				\centering
				\begin{tabular}{|c|c|c|}
					\hline
					$\lambda$  & Параллельное расписание & Последовательное расписание\\
					\hline
					$\frac{1}{8}$ & 0.1034 & 0.7213\\
					\hline
					$\frac{1}{4}$ & 0.0514& 0.7252\\
					\hline
					$\frac{3}{8}$ & 0.0325& 0.5082\\
					\hline
					$\frac{1}{2}$ & 0.0151& 0.3162\\
					\hline
					$\frac{5}{8}$ & 0.0121& 0.2048\\
					\hline
					$\frac{3}{4}$ & 0.0082& 0.1389\\
					\hline
					$\frac{7}{8}$ & 0.0049& 0.0845\\
					\hline
					1 & 0.0035& 0.0582\\
					\hline
				\end{tabular}
				\captionsetup{justification=centering}				
				\caption{Cреднее время работы алгоритма декодирования в секундах}
				\label{tab:lambda}
			\end{table}\par
			Видно, что с ростом $\lambda$ уменьшается время работы алгоритма, а параллельное расписание работает быстрее последовательного, в частности потому, что допускает лучшую векторизацию.\par
			Исследуем долю стабилизировавшихся beliefs (оценок маргинальных распределений) от номера итерации. Результаты приведены на рисунках \ref{fig:lambda_par} - \ref{fig:lambda_seq}:
			\begin{figure}[H]
				\centering
				\captionsetup{justification=centering}
				\includegraphics[width=0.9\textwidth]{lambda_par.eps}
				\caption{Параллельное расписание}
				\label{fig:lambda_par}
			\end{figure}
			\begin{figure}[H]
				\centering
				\captionsetup{justification=centering}
				\includegraphics[width=0.9\textwidth]{lambda_seq.eps}
				\caption{Последовательное расписание}
				\label{fig:lambda_seq}
			\end{figure}\par
			Видно, что при параллельном расписании алгоритм сходится быстрее. Также можно отметить, что выигрывают большие значения $\lambda,$ однако $\lambda = 1$ --- не лучшее из них, то есть учёт информации о предыдущих итерациях имеет смысл.\par
			Заметим, что графики немонотонны, то есть делать вывод о «стабилизации» belief'а на основании сравнения последних двух значений на самом деле нельзя, оно может соответствовать локальному минимуму.
		\subsection{Теорема Шеннона}
		\emph{Теорема Шеннона}\par
			\emph{$\forall r < c$ существует код, такой что вероятность ошибки декодирования стремится к нулю $p_{err} \rightarrow 0,$ когда длина блока стремится к бесконечности $n \rightarrow \infty.$}\par
		Проверим работу этой теоремы и исследуем зависимость характеристик кода (вероятности битовой и блоковой ошибки и расходимости алгоритма) от различных параметров. Вероятности будут приближены своей частотной оценкой в ходе математических испытаний методом Монте-Карло.
		\begin{enumerate}
			\item Зависимость от скорости $r$.\par
			Теорема Шеннона определяет пропускную способность канала как максимально допустимую скорость кода, при которой возможно осуществление надёжной коммуникации.\par
			Зафиксируем параметры $n = 200, q = 0.1, \lambda = \frac{7}{8}$ и исследуем эффективность кода (см. рис. \ref{fig:shannon_r}):
			\begin{figure}[H]
				\centering
				\captionsetup{justification=centering}
				\includegraphics[width=0.6\textwidth]{shannon_r.eps}
				\caption{Характеристики кода в зависимости от $r$}
				\label{fig:shannon_r}
			\end{figure}\par
			Действительно, после превышения пропускной способности канала блоковая ошибка становится равной 1.
			\item Зависимость от длины кодового слова $n$.\par
			Теорема Шеннона предполагает, что качество кода растёт при увеличении длины кодового слова $n.$\par
			Зафиксируем параметры $r = 0.3, q = 0.1, \lambda = \frac{7}{8}$ и исследуем эффективность кода (см. рис. \ref{fig:shannon_n}):
			\begin{figure}[H]
				\centering
				\captionsetup{justification=centering}
				\includegraphics[width=0.6\textwidth]{shannon_n.eps}
				\caption{Характеристики кода в зависимости от $n$}
				\label{fig:shannon_n}
			\end{figure}\par
			Действительно, в среднем с увеличением $n$ качество кода растёт.
			\item Зависимость от среднего количества единиц в столбце проверочной матрицы $j$.\par
			Одно из следствий теоремы Шеннона утверждает, что хорошими кодами являются коды со случайной проверочной матрицей $H,$ и, в частности, качество кода должно расти с увеличением среднего количества единиц в столбце этой матрицы.\par
			Проверим это утверждение; зафиксируем параметры $n =500, k = 60, q = 0.1, \lambda = \frac{7}{8}$ и исследуем эффективность кода (см. рис. \ref{fig:shannon_j}):
			\begin{figure}[H]
				\centering
				\captionsetup{justification=centering}
				\includegraphics[width=0.6\textwidth]{shannon_j.eps}
				\caption{Характеристики кода в зависимости от $j$\\ (err\_block совпадает с diver)}
				\label{fig:shannon_j}
			\end{figure}\par
			Однако получен обратный эффект, и с увеличением $j$ качество кода быстро падает. Возможно, это связано с тем, что в фактор-графе появляются циклы длины большей трёх, и это усложняет работу алгоритма.
		\end{enumerate}
		\subsection{Сравнение низкоплотностных кодов с БЧХ кодами}
			Коды Боуза --- Чоудхури --- Хоквингема [??] также являются линейными $(n, k)-$блоковыми кодами, сравним их с LDPC-кодами.
			\begin{enumerate}
				\item
				Зависимость от $r$.\\
				Зафиксируем параметры $n = 255, q = 0.1, \lambda = \frac{7}{8},$ и будем перебирать $k$ среди таких значений, что пара $(n, k)$ корретно задаёт код БЧХ.
			\begin{figure}[H]
				\begin{subfigure}[b]{0.5\textwidth}
					\centering
					\includegraphics[width=1.0\textwidth]{bch_r.eps}
					\caption{Вероятности битовой и блоковой ошибок}
					\label{fig:distr_c_a}
				\end{subfigure}
				\begin{subfigure}[b]{0.5\textwidth}
					\centering
					\includegraphics[width=1.0\textwidth]{bch_div.eps}
					\caption{Вероятности расходимости алгоритма}
					\label{fig:distr_c_b}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{Сравнение характеристик LDPC- и БЧХ-кодов в зависимости от $r$}
				\label{fig:distr_c_cond}
			\end{figure}
\par
			Ожидаемо, что при преодолении скоростью пропускной способности канала, вероятность блоковой ошибки становится равной 1.\par
			Видно, что LPDC-коды выигрывают у БЧХ по всем параметрам: вероятность ошибки и расходимости алгоритмов меньше. (В случае БЧХ под «расходимостью» понимаем отказ от декодирования.)
			\item
			Зависимость от $q$.\\
			Зафиксируем параметры $n = 255, k = 87, \lambda = \frac{7}{8}.$
			\begin{figure}[H]
				\begin{subfigure}[b]{0.5\textwidth}
					\centering
					\includegraphics[width=1.0\textwidth]{bch_q.eps}
					\caption{Вероятности битовой и блоковой ошибок}
					\label{fig:distr_c_a}
				\end{subfigure}
				\begin{subfigure}[b]{0.5\textwidth}
					\centering
					\includegraphics[width=1.0\textwidth]{bch_q_div.eps}
					\caption{Вероятности расходимости алгоритма}
					\label{fig:distr_c_b}
				\end{subfigure}
				\captionsetup{justification=centering}
				\caption{Сравнение характеристик LDPC- и БЧХ-кодов в зависимости от $q$}
				\label{fig:distr_c_cond}
			\end{figure}
			Очевидно, что с ростом вероятности ошибки в канале ухудшается и качество декодирования сообщений. В рассматриваемом случае LDPC вновь превзошли БЧХ.
			\end{enumerate}
	\section{Использованная литература}
		$[1]$ \\
		$[2]$ \\
		$[3]$ 
\end{document}